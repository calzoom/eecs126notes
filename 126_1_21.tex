\documentclass[11pt]{scrartcl}
\usepackage[header, margin, koma]{japjot}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[l]{EECS 126 Notes}
\fancyhead[r]{Japjot Singh}
\cfoot{\thepage}

\begin{document}
\title{\Large EECS 126 Notes}
\author{\large Japjot Singh}
\date{\large\today}

\maketitle

Course notes for EECS 126 taken in Spring 2020. 

\tableofcontents
\newpage

\section{Tuesday, January 21} 
Understand problem as an "experiment" and then solve it using tools in your
skillset: combinatorics, calculus, common sense.
\subsection{Fundamentals}
\begin{definition}
	Sample Space $\Omega$ of an experiment is the set of all oucomes of
the experiment. 
\end{definition}

\begin{example}
	Your experiment is 2 fair coins $\Omega = \{HH, HT, TH, TT\}$
these outcomes (base outcomes) are \textbf{mutually exclusive (ME) and
collectively exhaustive (CE)} 
\end{example} 

\begin{example}
Toss a coin till the first "Heads" $\Omega = \{H, TH, TTH,
\ldots\};|\Omega|= \infty$
\end{example} 

\begin{example}
Waiting  at the bus-stop for next bus $\Omega = (0, T)$
\end{example} 

Visual 1 - We have the experiment which produces outcomes, once you have the
outcome space the next definition is the definition of events

\begin{definition}[Events]
Allowable subsets of $\Omega$ (collections of
outcomes) 
\end{definition}

\begin{example}
Get at least 1 Head in experiment 1, $\{HH, HT, TH\}$,
$p=\frac{3}{4}$ 
\end{example} 

\textbf{Defining events carefully is the key to tackling many tough problems.}

\begin{example}
Ex 2.2: Get an even number of tosses
\end{example} 

\begin{example}
Ex 2.3: Waiting time < 5 min
\end{example} 

\begin{definition}[Probability Space]
	 A \textbf{probability space} $(\Omega, \mathcal{F} , \mathcal{P} )$ is a mathematical construct
to model "experiments" and has 3 components: \begin{enumerate}
  \item $\Om$ is the set of all possible outcomes
  \item $\mathcal{F}$ set of all events (composition of outcomes), where each event is a set containing
    0 or more base outcomes, \textbf{$\emptyset$ is a base outcome} where $\pr(\emptyset) = 0$.
      $\mathcal{F} $ is intuitively a powerset
    (i.e. for the experiment in example 1.1 $\mathcal{F}=\{\emptyset, \{H,H\}, \{H, T\},
    \ldots\}$).
    \item $\mathcal{P}$ is the proability measure which assigns a number in $[0, 1]$ to each event in $\mathcal{F}$.
\end{enumerate}
\end{definition}

\textbf{Base oucomes must be ME and CE} that is when writing out $\Om$ as a collection of all the base outcomes, they should be the most simplified components.

\subsection{Axioms of Probability (Kolmogov)}
What properties do we need the probability measure $\mathcal{P} $ to satisfy?
\begin{enumerate}
    \item $\pr(\emptyset)=0$ 
    \item $\pr(\Omega)=1$, really just a normalization
    \item $\pr(A_1\cup A_2\cup\ldots)=P(A_1)+P(A_2)+\ldots$ for disjoint (ME)
      events $A_1,A_2,\ldots$
\end{enumerate}
for disjoint $\pr(\bigcup_{i=1}^n A_{i} )= \sum_{i=1}^n \pr(A_{i} )$ 
\subsection{Fundamental facts about probability} 
\begin{enumerate}
    \item $\pr(A^{c}) = 1 - \pr(A)$ 
    \item $\pr(A\cup B) =  \pr(A)+\pr(B)-\pr(A\cap B)$ \textbf{Vis2 Venn
      Diagram}  
      \item Union-bound $\pr(A_1\cup A_2\cup\ldots\cup A_{n} )\leq \sum_{i=1}
        ^{n} \pr(A_{i} )$
      \item Inclusion-Exclusion, a generalized version of number 2
\end{enumerate}

\begin{theorem}[Inclusion-Exclusion]
	$\pr(A_1\cup A_2\cup\ldots\cup A_{n} )= \sum_{k=1}^n \sum_{1\leq i_1} \sum_{\leq i_2}\ldots\sum_{i_{k} \leq
		n} (-1)^{i+1}$
\end{theorem}
\begin{proof}
	here
\end{proof}

\subsection{Discrete Probability} 
\[
\pr(A) = \sum_{\omega \in  A} \pr(\omega)
\] 
In a uniform sample space, all the outcomes are equally likely so then \[
  \pr(A) = \frac{|A|}{|\Omega|}
\]
\subsection{Conditional Probability} 
Similar to events, conditioning on the right event will bail you out of tricky problems.
\begin{definition}
  $P(A|B) := \pr(\text{Event A given that Event B has occured})$ 
\end{definition} 
Thus for any event $A$ if $\pr(B)\neq 0$, \[
\pr(A|B) = \frac{\pr(A\cap B)}{\pr(B)}\] 
\begin{example}
  Consider 2 six-sided dice. Let $A$ be the event that the first dice rolls is
  a 6. Let  $B$ be the event the sum of the two dice is 7. Then \[
    \pr(A|B) = \frac{\pr(A\cap B)}{\pr(B)} = \frac{\pr(\{6,1\})}{\pr(\{6,1\},
    \{5,2\},\ldots,\{1,6\})}
  \]
  Similarly $\pr(A|\text{sum is 11})=\frac{1}{2}$
\end{example}
When conditioning on $B$,  $B$ becomes to new  $\Om$. 

\subsection{Product (Multiplication) Rule} 
\subsection{Total Probability}
\subsection{Bayes' Theorem} 
\newpage
\section{Thursday, January 23}
\subsection{Announcement}
Readings B\&T ch1 and 2, HW 1 due next wednesday one minute before midnight 

\subsection{Birthday Paradox} 
Assuming a group of $n$ individuals whose birth dates are distributed uniformly
at random. Given $k=365$ days in the year what is the probability that at least
2 people in the group share the same birthday. Our sample space is the consists
  of each possible set of assignments of birth dates to the  $n$ students in
  the class. Since there are 365 posisble days for each of the  $n$ students in
  the group $|\Om| = k^{n} = 365^{n} $. Now we can define our event of
  interest, $A$, that at least 2 people have the same birthday. Since this is
  a hard event to work with we can look at the complement $A^{c} $ the event
  that no two people share a birth date. We can reach the solution with
  a counting argument
  \[
    \pr(A^{c}) = \frac{|A^{c}|}{|\Om|} = \frac{365 * 364 *\ldots*(365
    - (n-1))}{365^{n} }
  \]
  or with a probabilistic argument using the chain rule \[
    \pr(A^{c} ) = 1(1-\frac{1}{k})(1-\frac{2}{k})\cdots(1 - \frac{n-1}{k})
  \]
  the latter expression can be approximated using Taylor Series which say
  $e^{x} \approx 1 + x $ for $|x|<<1$.  \[
    \pr(A^{c} )\approx 1\cdot e^{-\frac{1}{k}} \cdot e^{-\frac{2}{k}} \cdots
    e^{-\frac{n-1}{k}} 
  \] thus $\pr(A)=1-\pr(A^{c} )\approx 1 - e^{-\frac{n^2}{2k}} $ 

\subsection{Bayes Rule False Positive Problem} 
Suppoes there is a new test for a rare disease. 
\begin{itemize}
  \item If a person has the disease, test positive with $p=0.95$
     \item If person does not have disease, test negative with $p=0.95$ 
       \item Random person has the diseaes with $p=0.001$
\end{itemize}
Suppose a person tested positive, what is the probability that person has the
disease. Let $A$ be the event has disease and  $B$ be the event test positive
then by applying Bayes Rule directly  \[
  \pr(A|B) = \frac{(0.95)(0.001)}{(0.95)(0.001) + (0.999)(0.05)} = 0.1875
\] the factor heavily contributing to this number is the prior, how rare the
diesase is in the first place. 
\subsection{Independence} 
\begin{definition}
  Two events are independent if the occurence of one provides \textbf{no
  information} about the occurence of the other (i.e. $\pr(A|B) = \pr(A)$). 
\end{definition}
\textbf{insert vis1}  
Indepedence can also be written as \[
  \pr(A\cap B) = \pr(A)\pr(B)
\] 
\textbf{Note:} Disjoint events are \textbf{not} Independent. Events $A$ and
$B$ are disjoint if and only if $\pr(A\cap B) = 0 \implies  \pr(A)=0 \lor
\pr(B)=0$. Thus since Base outcomes of a random experiment are disjoint (ME)
and have non-zero probabilities they \textbf{must be dependent}.

\subsubsection{Conditional Independence} 
\[
\pr(A\cap B|C) = \pr(A|C) \cdot \pr(B|C)
\] 
Note that \begin{itemize}
  \item Dependent events can be conditionally independent
    \item Independent events can be conditionally dependent
\end{itemize}
\begin{example}
  Consider 2 indistinguishable coins: one is two-tailed and the other is
  two-headed. You pick on of the 2 coins at random and flip it twice. \\\\
  Let $H_{i}$ be the event that the $i^{th} $ flip is a Head ($i=1,2$). By
  itself $\pr(H_1)=\pr(H_2) = \frac{1}{2}$ and $\pr(H_2|H_1) = 1 \neq
  \pr(H_2) = 1/2$. Furthermore, $\pr(H_1\cap H_2|A) = \pr(H_1|A)\pr(H_2|A\cap
  H_1)= \pr(H_1|A)\pr(H_2|A)$ which by definition tells us that $H_1,H_2$ are
  conditionally independent given $A$.
\end{example}

\subsubsection{Independence of a collection of events}
For all possible subsets of your events $A_{1:n} $, each subset must be
independent that is
\[
  \pr(\bigcap_{i\in S}A_{i}) = \prod_{i\in S}(\pr(A_{i} )), \forall S
\] where $S$ is any subset of the collection of events. Pairwise independence
\textbf{does not imply} Joint independence of 3 or more events. 
\newpage
\section{Markov Chains}
Interested in models where the effect of the past on the future is summarized
by a state, which changes over time given probabilities.
\subsection{Discrete Time Markov Chains}
In \textbf{discrete-time Markov chains}, state changes at certain discrete time
instants, indexed by an integer variable $n$. At each step $n$, the state of
the chain is denoted $X_{n}$, and belongs to a \textbf{finite} set $\mathcal{S}$ of
possible states, called the state space. WLOG let $S = \{1,\ldots,m\}$.The Markov Chain is described in terms
of its transition probabilities $p_{ij}$ where \[
  p_{ij} = \pr(X_{n+1} = j | X_{n}=i) ~~ i,j \in \mathcal{S}
\] 
The key assumption underlying these chains is that the transition probabilities
apply whenever $i$ is visited, no matter what happened in the past, and no
matter how $i$ was reached, formally this is the \textbf{Markov property},
requiring that: \[
  \pr(X_{n+1} = j | X_{n}=i, X_{n-1}=i_{n-1},\ldots,X_0=i_0) = \pr(X_{n+1}=j
  | X_{n}=i) = p_{ij}
\] Furthermore the transition probabilities $p_{ij}$ must be nonnegative, and
sum to one: \[
  \sum_{j=1}^{m}  p_{ij} = 1, ~\text{for all }i
\] 
Another efficient way to encode the MC chain model is a transition probability
matrix, a 2D array whose element at row $i$ and column $j$ is $p_{ij}$, the
transition probability from $i$ to $j$ \[
P = \begin{bmatrix}
  p_{11} & p_{12} & \cdots & p_{1m} \\
  p_{21} & p_{22} & \cdots & p_{2m} \\
  \vdots & \vdots & \vdots & \vdots \\
  p_{m 1} & p_{m 2} & \cdots & p_{mm}
\end{bmatrix}
\] Note that the transition matrix format is (row, col), $(i,j)$, (from,
to).\\\\There is often a need to introduce new states that capture the depedence of the
future on the model's past history. The probability of any single path can be
found simply using the multiplication rule and tracing the path of transition
probabilities. If there is no conditioning on the first state then we need to
specify a probability law for the initial state $X_0$, the initial
distribution.
\subsubsection{$n$-Step Transition Probabilities}
Many problems require calculating the probability law of the state at some
future tiem, conditioned on the current state. This probability law is captured
by the $n$\textbf{-step transition probabilities}, defined by  \[
  r_{ij}(n) = \pr(X_{n}=j | X_0 = i)
\] In words, $r_{ij}(n)$  is the probability that the state after $n$  time
periods will be $j$, give that the current state is $i$. We can calculate it
using the following recursion, \textbf{Chapman-Kolmogorov equation}  
\begin{theorem}[Chapman-Kolmogorov Equation for $n$-Step Transition
  Probabilities]
  The $n$ -step transition probabilities can be calculated using the formula \[
    r_{ij}(n) = \sum_{k=1}^{m} r_{ik}(n-1)p_{kj}, ~~~n>1, \text{and all }i,j
  \] starting with $r_{ij}(1) = p_{ij}$
  \begin{proof}
    \begin{align*}
      \pr(X_{n}= j | X_0 = i) &= \sum_{k=1}^{m} \pr(X_{n-1}=k | X_0
      =i)\pr(X_{n} = j | X_{n-1}=k, X_0=i) \\ &= \sum_{k=1}^{m} r_{ik}(n-1)
    \end{align*}
  \end{proof}
The Chapman-Kolmogorov equation can be represented my concisely via matrix
multiplication, specifically the matrix of $n$-step transition probabilities
$r_{ij}(n)$ is obtained by multiplying the matrix of $(n-1)$-step transition
probabilities $r_{ik}(n-1)$, with the one-step transition probability matrix.
Thus the $n$-step transition probability matrix is just the $n$th power of the
transition probability matrix, $P^{n} $.
\end{theorem}
As $n\to \infty$, if $r_{ij}(n)$ converges to a limit and this limit does not
depend on initial state $i$, then we say that state $j$ has a positive
""steady-state" probability of being occupied at times far into the future.
However, there are examples of qualitatively different behavior: where
$r_{ij}(n)$ converges, but the limit depends on the initial state, and can be
zero for selected states, particularly the probability that a particular
absorbing state will be reached depends on how "close" we start to that state.
This illustrates that there is a variety of states and asymptotic occupancy
behavior in Markov chains. Thus we are motivated to classify and analyze
various possibilities. 
\subsection{Classification of States}
We begin by focusing on the mechanism by which some states after being visited
once, are certain to be visited again, while for other states this may not be
the case. Our goal is to classify the states of a Markov chain with a focus on
the long-term frequency by which they are visited. Let us first make the notion
of revisiting a state precise. A state $j$ is \textbf{accessible} from state $i$
if for some $n$, the $n$-step transition probability $r_{ij}(n)$ is positive
(there is a positive probability of reaching $j$, starting from $i$, after some
number of time steps). Let $A(i)$ denote the set of states accessible from
state $i$.
\begin{definition}[recurrent]
  State $i$ is \textbf{recurrent} if \emph{for every} $j$ that is accessible
from $i$, $i$ is also accessible from $j$, that is $\forall j \in A(i)$, $i\in
A(j)$. 
\end{definition}
If we start at a recurrent state $i$, we can only visit $j\in A(i)$. But since
state $i$ is recurrent $i\in A(j)$. Thus, from any future state, there always
some probability of returning to state $i$. Given enough time, this is certain
to happen. By repeating this argument, if a recurrent state is visited once, it
is certain to be revisited an infinite number of times. \textcolor{red}{does
this hold only if state $j$ is itself recurrent? like what if you transition to
state $j$ but the from state $j$ you can transition into some absorbing state
$k$, then there is a nonzero probabilty of not returning to state $i$, which
means that you are note certain to revist $i$ an infinite number of times}.\\\\
A state is called \textbf{transient} if it is not recurrent. Thus, a state
$i$ is transient if there is a state $j\in A(i)$ such that $i$ is not
accessible from $j$, that is there exists $j\in A(i)$ but $i\\not\in  A(j)$.
After each visit to state $i$, there is a positive probability that the state
enteres such a state $j$ from which $i$ is no longer accessible. Given enough
time, this will hapen, and state $i$ cannot be visited after that. Thus
a transiet state will only be viisted a a finite number of times. 
\begin{definition}[recurrent class]
  If $i$ is a recurrent state, the set of states $A(i)$ that are accessible
  form a \textbf{recurrent class} (or simply \textbf{class}). This means that
  the states in $A(i)$ are all accessible from each other, and no state outside
  $A(i)$ is accessible from them. Mathematically, for a recurrent state $i$, we
  have $A(i) = A(j)$ for all $j$ that belong to $A(i)$. 
\end{definition}
At least one recurrent state must be accessible from any transient state, this
follows from the definition of transient state. It follows that there must
exist at least one recurrent state and hence at least one class, giving the
following result
\begin{theorem}[Markov Chain Decomposition]
  \begin{itemize}
    \item A MC can be decomposed into one or more recurrent classes, plus
      possibly some transient states
      \item  A recurrent state is accessible for all states in its class, but
        is not accesssible from \emph{recurrent} states in \emph{other classses}
        \item  A transient state is not accessible from any recurrent state,
          but a a recurrent state must be accessible from a transient state
          (otherwise the state cannot be transient it would just be recurrent
          with itself)
          \item  At least one, possibly more, recurrent states are accessible
            from a given transiet state, this follows directly from the
            previous bullet 
  \end{itemize}
\end{theorem}
The previous theorem implies the following:
\begin{enumerate}
  \item  once the state enters (or starts in) a class of recurrent states, it
    stays within that class; since all states in the recurrence class are
    accessible from each other, all states in the class will be visited
    an infinite number of times
    \item  if the initial state is transient, then the state trajectory
      contains an initial portion consisting of transient states and a final
      portion consisting of recurrent states from the same class
\end{enumerate}
\begin{example}
  In a MC at least one recurrent state must be accssible from any given state.
  That is, for any $i$, there is at least one recurrent $j$ in the set $A(i)$. 
  \begin{proof}
    \textcolor{red}{TODO} 
  \end{proof}
\end{example}
\begin{example}
  Show that if a recurrent state is visited once, the probability that it will
  be visited gain in the future is equal to 1 (and, therefore, the probability
  that it will be visited an infinte number of times is equal to 1).
  \begin{proof}
    Let $s$ be a recurrent state, and suppoes $s$ has been visited once. From
    then on, the only possible states are those in the same recurrence class as
    $s$. Therefore WLOG we can assume there is a single self-absorbing
    state $s$. We now want to show from some current state $i\neq s$,
    $s$ is guaranteed to be visited some time in the future.\\\\ Consider a new
    MC with a self-absorbing state $s$. The transitions out of states $i$,
    $i\neq s$ are unaffected. Clearly, $s$ is recurrent in the new chain.
    Furthermore, for any $i\neq s$, there is a positive probability path from
    $i$ to $s$ in the original chain (since $s$ is recurrent in the original
    chain), this holds true in the new chain \textcolor{red}{still not
    completely convinced of this}. Since $i$ is not accessible from $s$ in the
    new chain, it follows that  every $i\neq s$ in the new chain is transient.
    But if there exist recurrent states in a MC, they will eventually be
    visited, so the state $s$ will eventually be visited by the new chain (with
    probability 1). But the original chain is identical to the new one until
    the time that $s$ is first visited. Hence, state $s$ is guaranteed to be
    eventually visited by the original chain. BY repeating this argument, we
    see that $s$ is guaranteed to be visited an infinite number of times (with
    probability 1).
  \end{proof}
\end{example}

\subsubsection{Periodicity}

\subsection{Recurrence and Transience}
For each $x\in \mathcal{X} $, the random variable $T_{x} $ represents the first
time that the chain visits state $x$, that is $T_{x}:= \min \{n\in \N: X_{n}
  = x $. $T_{x} $ is called the hitting time of state $x$. We further define
  $T_{x}^{+}  $ which is the hitting time for state $x$ execept $T_{x}^{+}
  $ cannot equal 0, to avoid the trivial condition in which the chain starts at
  $x$. 
  Let $\rho_{x,y}:=\pr_x(T_{y}^{+} < \infty  )$, the probability that starting
  from state $x$ we eventually reach state $y$ and shorthand
  $\rho_{x}=\rho_{x,x}  $. 
  \begin{definition}
    A state $x$ is \textbf{recurrent}  if $\rho_{x}=1 $ and \textbf{transient}
  if $\rho_{x}<1 $.
  \end{definition}
  \begin{proposition}
    Let $N_{x} $ be the total number of visits to state $x$. If $x$ is
    recurrent then $N_{x}=\infty $ in probability almost surely. Thus
    $\mathbb{E}_{x} [N_{x} ] = \mathbb{E} [N_{x} | X_{0}=x  ] =\infty$. On
    the other hand, if $x$ is transient then $\mathbb{E}_{x} [N_{x}
    ] < \infty$, in fact, \[
      \mathbb{E}_{x} [N_{x} ]  = \frac{\rho_{x}}{1-\rho_{x}}
    \]  
  \end{proposition}
\end{document}
