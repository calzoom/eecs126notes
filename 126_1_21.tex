\documentclass[11pt]{scrartcl}
\usepackage[header, margin, koma]{japjot}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[l]{EECS 126 Notes}
\fancyhead[r]{Japjot Singh}
\cfoot{\thepage}

\begin{document}
\title{\Large EECS 126 Notes}
\author{\large Japjot Singh}
\date{\large\today}

\maketitle

Course notes for EECS 126 taken in Spring 2020. 

\tableofcontents
\newpage

\section{Tuesday, January 21} 
Understand problem as an "experiment" and then solve it using tools in your
skillset: combinatorics, calculus, common sense.
\subsection{Fundamentals}
\begin{definition}
	Sample Space $\Omega$ of an experiment is the set of all oucomes of
the experiment. 
\end{definition}

\begin{example}
	Your experiment is 2 fair coins $\Omega = \{HH, HT, TH, TT\}$
these outcomes (base outcomes) are \textbf{mutually exclusive (ME) and
collectively exhaustive (CE)} 
\end{example} 

\begin{example}
Toss a coin till the first "Heads" $\Omega = \{H, TH, TTH,
\ldots\};|\Omega|= \infty$
\end{example} 

\begin{example}
Waiting  at the bus-stop for next bus $\Omega = (0, T)$
\end{example} 

Visual 1 - We have the experiment which produces outcomes, once you have the
outcome space the next definition is the definition of events

\begin{definition}[Events]
Allowable subsets of $\Omega$ (collections of
outcomes) 
\end{definition}

\begin{example}
Get at least 1 Head in experiment 1, $\{HH, HT, TH\}$,
$p=\frac{3}{4}$ 
\end{example} 

\textbf{Defining events carefully is the key to tackling many tough problems.}

\begin{example}
Ex 2.2: Get an even number of tosses
\end{example} 

\begin{example}
Ex 2.3: Waiting time < 5 min
\end{example} 

\begin{definition}[Probability Space]
	 A \textbf{probability space} $(\Omega, \mathcal{F} , \mathcal{P} )$ is a mathematical construct
to model "experiments" and has 3 components: \begin{enumerate}
  \item $\Om$ is the set of all possible outcomes
  \item $\mathcal{F}$ set of all events (composition of outcomes), where each event is a set containing
    0 or more base outcomes, \textbf{$\emptyset$ is a base outcome} where $\pr(\emptyset) = 0$.
      $\mathcal{F} $ is intuitively a powerset
    (i.e. for the experiment in example 1.1 $\mathcal{F}=\{\emptyset, \{H,H\}, \{H, T\},
    \ldots\}$).
    \item $\mathcal{P}$ is the proability measure which assigns a number in $[0, 1]$ to each event in $\mathcal{F}$.
\end{enumerate}
\end{definition}

\textbf{Base oucomes must be ME and CE} that is when writing out $\Om$ as a collection of all the base outcomes, they should be the most simplified components.

\subsection{Axioms of Probability (Kolmogov)}
What properties do we need the probability measure $\mathcal{P} $ to satisfy?
\begin{enumerate}
    \item $\pr(\emptyset)=0$ 
    \item $\pr(\Omega)=1$, really just a normalization
    \item $\pr(A_1\cup A_2\cup\ldots)=P(A_1)+P(A_2)+\ldots$ for disjoint (ME)
      events $A_1,A_2,\ldots$
\end{enumerate}
for disjoint $\pr(\bigcup_{i=1}^n A_{i} )= \sum_{i=1}^n \pr(A_{i} )$ 
\subsection{Fundamental facts about probability} 
\begin{enumerate}
    \item $\pr(A^{c}) = 1 - \pr(A)$ 
    \item $\pr(A\cup B) =  \pr(A)+\pr(B)-\pr(A\cap B)$ \textbf{Vis2 Venn
      Diagram}  
      \item Union-bound $\pr(A_1\cup A_2\cup\ldots\cup A_{n} )\leq \sum_{i=1}
        ^{n} \pr(A_{i} )$
      \item Inclusion-Exclusion, a generalized version of number 2
\end{enumerate}

\begin{theorem}[Inclusion-Exclusion]
	$\pr(A_1\cup A_2\cup\ldots\cup A_{n} )= \sum_{k=1}^n \sum_{1\leq i_1} \sum_{\leq i_2}\ldots\sum_{i_{k} \leq
		n} (-1)^{i+1}$
\end{theorem}
\begin{proof}
	here
\end{proof}

\subsection{Discrete Probability} 
\[
\pr(A) = \sum_{\omega \in  A} \pr(\omega)
\] 
In a uniform sample space, all the outcomes are equally likely so then \[
  \pr(A) = \frac{|A|}{|\Omega|}
\]
\subsection{Conditional Probability} 
Similar to events, conditioning on the right event will bail you out of tricky problems.
\begin{definition}
  $P(A|B) := \pr(\text{Event A given that Event B has occured})$ 
\end{definition} 
Thus for any event $A$ if $\pr(B)\neq 0$, \[
\pr(A|B) = \frac{\pr(A\cap B)}{\pr(B)}\] 
\begin{example}
  Consider 2 six-sided dice. Let $A$ be the event that the first dice rolls is
  a 6. Let  $B$ be the event the sum of the two dice is 7. Then \[
    \pr(A|B) = \frac{\pr(A\cap B)}{\pr(B)} = \frac{\pr(\{6,1\})}{\pr(\{6,1\},
    \{5,2\},\ldots,\{1,6\})}
  \]
  Similarly $\pr(A|\text{sum is 11})=\frac{1}{2}$
\end{example}
When conditioning on $B$,  $B$ becomes to new  $\Om$. 

\subsection{Product (Multiplication) Rule} 
\subsection{Total Probability}
\subsection{Bayes' Theorem} 
\newpage
\section{Thursday, January 23}
\subsection{Announcement}
Readings B\&T ch1 and 2, HW 1 due next wednesday one minute before midnight 

\subsection{Birthday Paradox} 
Assuming a group of $n$ individuals whose birth dates are distributed uniformly
at random. Given $k=365$ days in the year what is the probability that at least
2 people in the group share the same birthday. Our sample space is the consists
  of each possible set of assignments of birth dates to the  $n$ students in
  the class. Since there are 365 posisble days for each of the  $n$ students in
  the group $|\Om| = k^{n} = 365^{n} $. Now we can define our event of
  interest, $A$, that at least 2 people have the same birthday. Since this is
  a hard event to work with we can look at the complement $A^{c} $ the event
  that no two people share a birth date. We can reach the solution with
  a counting argument
  \[
    \pr(A^{c}) = \frac{|A^{c}|}{|\Om|} = \frac{365 * 364 *\ldots*(365
    - (n-1))}{365^{n} }
  \]
  or with a probabilistic argument using the chain rule \[
    \pr(A^{c} ) = 1(1-\frac{1}{k})(1-\frac{2}{k})\cdots(1 - \frac{n-1}{k})
  \]
  the latter expression can be approximated using Taylor Series which say
  $e^{x} \approx 1 + x $ for $|x|<<1$.  \[
    \pr(A^{c} )\approx 1\cdot e^{-\frac{1}{k}} \cdot e^{-\frac{2}{k}} \cdots
    e^{-\frac{n-1}{k}} 
  \] thus $\pr(A)=1-\pr(A^{c} )\approx 1 - e^{-\frac{n^2}{2k}} $ 

\subsection{Bayes Rule False Positive Problem} 
Suppoes there is a new test for a rare disease. 
\begin{itemize}
  \item If a person has the disease, test positive with $p=0.95$
     \item If person does not have disease, test negative with $p=0.95$ 
       \item Random person has the diseaes with $p=0.001$
\end{itemize}
Suppose a person tested positive, what is the probability that person has the
disease. Let $A$ be the event has disease and  $B$ be the event test positive
then by applying Bayes Rule directly  \[
  \pr(A|B) = \frac{(0.95)(0.001)}{(0.95)(0.001) + (0.999)(0.05)} = 0.1875
\] the factor heavily contributing to this number is the prior, how rare the
diesase is in the first place. 
\subsection{Independence} 
\begin{definition}
  Two events are independent if the occurence of one provides \textbf{no
  information} about the occurence of the other (i.e. $\pr(A|B) = \pr(A)$). 
\end{definition}
\textbf{insert vis1}  
Indepedence can also be written as \[
  \pr(A\cap B) = \pr(A)\pr(B)
\] 
\textbf{Note:} Disjoint events are \textbf{not} Independent. Events $A$ and
$B$ are disjoint if and only if $\pr(A\cap B) = 0 \implies  \pr(A)=0 \lor
\pr(B)=0$. Thus since Base outcomes of a random experiment are disjoint (ME)
and have non-zero probabilities they \textbf{must be dependent}.

\subsubsection{Conditional Independence} 
\[
\pr(A\cap B|C) = \pr(A|C) \cdot \pr(B|C)
\] 
Note that \begin{itemize}
  \item Dependent events can be conditionally independent
    \item Independent events can be conditionally dependent
\end{itemize}
\begin{example}
  Consider 2 indistinguishable coins: one is two-tailed and the other is
  two-headed. You pick on of the 2 coins at random and flip it twice. \\\\
  Let $H_{i}$ be the event that the $i^{th} $ flip is a Head ($i=1,2$). By
  itself $\pr(H_1)=\pr(H_2) = \frac{1}{2}$ and $\pr(H_2|H_1) = 1 \neq
  \pr(H_2) = 1/2$. Furthermore, $\pr(H_1\cap H_2|A) = \pr(H_1|A)\pr(H_2|A\cap
  H_1)= \pr(H_1|A)\pr(H_2|A)$ which by definition tells us that $H_1,H_2$ are
  conditionally independent given $A$.
\end{example}

\subsubsection{Independence of a collection of events}
For all possible subsets of your events $A_{1:n} $, each subset must be
independent that is
\[
  \pr(\bigcap_{i\in S}A_{i}) = \prod_{i\in S}(\pr(A_{i} )), \forall S
\] where $S$ is any subset of the collection of events. Pairwise independence
\textbf{does not imply} Joint independence of 3 or more events. 
\newpage
\section{Poisson Process}
If a bus arrives every every 15 minutes, with full certainty, then a passenger
showing up at a random time will have to wait on average $\frac{1}{16}(0
+1+\ldots + 14 + 15) = \frac{1}{2}(0 + 15) = 7.5$ minutes. This is because
with equal probability you could arrive when the first bus, or when the second
bus comes, or any time in between and it is deterministic that each bus arrives
every 15 minutes.\\\\ Now let's introduce uncertainty. Suppose the arrival of
buses is modeled by a Poisson Process with rate $\lambda=\frac{1}{15}$. This
means that we expect 1 arrival every $\lambda x = 1 \to x = \frac{1}{\lambda
} = 15$ units of time (minutes). This means that interarrival
time $T_{i} \sim \exp(15)$ and $\mathbb{E}[T_{i}]=\frac{1}{15}$. That is, a bus arrives
every 15 minutes, with some uncertainty. \\\\ Furthermore the time-homogeneity
($\pr(k, \tau) = \pr(k, t), ~\forall \tau=t$),
independence ($N_{\tau}\sim Poisson(\lambda \tau)$, specifically $N_{\tau}$ is
memoryless, that is for any time $t>0$ the history of the process after time $t$ is
also a Poisson process and is indepdent from the history of the process until
time $t$ this is true in reverse-time direction as well), and small interval
properties hold ($\pr(0, \tau) = 1-\lambda \tau, ~ \pr(1, \tau) = \lambda \tau,
~ \pr(k, \tau) = \mathcal{O}(\epsilon)$). \\\\ We can illustrate this as
a limiting case of a discrete setup for better intuition of the properties.
Imagine rolling a 15-face die, one face colored green indicating a bus arrival and
14 colored red indicating the absence of an arrival. How often should I roll
   this dice to simulate the process. The probability of a green face
   is $\frac{1}{15}$. Let $T$ equal the number of trials until we see a green
   face. Since each roll of the dice is $\sim Ber(\frac{1}{15})$,
   $T\sim Geo(\frac{1}{15})$ and $\mathbb{E}[T]=15$. We expect to roll 15
   times to observe one success. How often should we roll to reach our target
   success rate of $\lambda = \frac{1}{15} = \frac{1\text{ success }}{15\text{
   minutes}}$ (we can replace minutes with any units but in the question we
   were given that the rate was define in terms of minutes).  So we just need
   to solve \begin{align*}
     \frac{1\text{ success}}{15\text{ rolls }}\cdot \frac{1\text{ roll
     }}{x\text{ minutes}} &= \frac{1 \text{ success}}{15\text{ minutes}}
     = \lambda \\
                          &\rightarrow x =1
   \end{align*} If we roll once every minute, then we will expect to see one
   arrival every 15 minutes. Now let us consider a 60 sided dice with one face
   green and the rest red. Then $T\sim Geo(\frac{1}{60})$ and $\mathbb{E}[T]
   = 60 $ implying $\frac{1}{60} \frac{\text{success}}{\text{rolls}}$ and thus
   \begin{align*}
     \frac{1}{60}\frac{\text{arrival}}{\text{rolls}} \cdot
     \frac{1}{x}\frac{\text{roll}}{\text{minutes}} &=
     \frac{1}{15}\frac{\text{arrival}}{\text{minutes}} \\ &\implies
     x = \frac{1}{4}
   \end{align*} If we roll once every 15 seconds, $0.25$ minutes, then we will
   expect to see one arrival every 15 minutes. \\\\ Now we can consider a $N$
   sided dice. $T\sim Geo(\frac{1}{N})$ and $\mathbb{E}[T] = N $, so we expect
   one arrival every $m$ units (minutes). We need $Nm=15$ and so
   $m=\frac{15}{N}$ verified below \begin{align*}
     \frac{1}{N}\frac{\text{arrivals}}{\text{trials}} \cdot
     \frac{1}{x}\frac{\text{trials}}{\text{units}} &=
     \frac{1}{15}\frac{\text{arrivals}}{\text{units}} \\ &\to x = \frac{15}{N}
     \\
     \lim_{N\to \infty} \frac{1}{\lambda N} &\to 0
   \end{align*} That is as we let this dice contain an infinite number of
   sides, the probability of an arrival at an instant goes to 0, but also the
   rate at which we sample for arrivals gets faster and faster and we expect to
   see one arrival every $\frac{1}{\lambda }$ units. This is the continuous
   time Poisson process. \\\\ Notice how the properties of continuous
   probability and PP line up. \textcolor{red}{is there a correspondance with
   density, probability at a single point being 0 and then delta getting
 smaller corresponding with the interval becoming smaller?}. The PP properties
 also hold and rather intuitively. Since each roll of the dice is a Bernoulli
 trial, each sample is iid. Indepedence follows directly from the fact that the
 number of arrivals during an interval is the sum of the iid samples in that
 interval and since functions of indepdent variables are indepdent, the nmber
 of arrivals in disjoint intervals are indepedent. And since they are
 identically distributed, time-homogeneity holds. $\pr(1,\tau)$ can be written
 as some quantity times the number of intervals of unit-length $m$. In a length
 $\tau$ interval there are $\frac{\tau}{m}$ samples from arrivals. In each of
 these samples the probability of arrival is some $a$. We want to find $a$ such
 that $a\cdot \frac{\tau}{m} = \lambda \tau$. We find that such $a$ satisifies
 $\frac{m}{a}= \frac{1}{\lambda}$ and we use the fact that $Nm=\frac{1}{\lambda
 }$ from the bottom of the previous page along with $\lim_{N\to \infty} m = 0$
 to get $a=\frac{1}{N}$ and the interpretation that in the limit as $N\to
 \infty$ we approach and infinite number samples from arrivals where each
 sample is an arrival with probability $\frac{1}{N}$. That is in a PP the
 probability of one arrival in a length $\tau$ interval is equivalent to
 integrating an infinitely small uniform density across a relatively large
 region with respect to the rate parameter that the proportion of arrivals to
 unit-length intervals is equal to $\lambda $. The probability of more than one
 arrival in an interval of length $\tau$ also approach 0 as $\tau$ approaches
 0. \\\\ Thus our intuition: A Poisson process with rate $\lambda $ models
    arrivals over a period of time where the rate indicates how many
    unit-length intervals $\delta$ there are for each arrival. So for some time
    interval $\tau$. The probability of $k$ arrivals in this interval $\tau$,
    is effectively equal to the probability of $k$ successful realizations of
    $$\lim_{N\to \infty}\frac{\tau}{\delta} Bernoulli(1/N)$$ 
    In cleaner notation: $X_{i}\sim Bernoulli(\frac{1}{N})$ models a arrival or
    non-arrival in each sample. We sample a total of $\frac{\tau}{\delta}$
    times, or the total number of unit interval times since $X_i$ models
    arrivals in unit intervals. Furthemore we are modeling 
    $N\delta = \frac{1}{\lambda }$  we have $$\lim_{N\to\infty}\delta =0$$ an
    inverse relationship between $N $ and $\delta$. Then the number of arrivals
    of the interval of length $\tau$ is just $S_{\tau} = X_1 + X_2 + \ldots
    + X_{\frac{\tau}{\delta}}$. And so $\pr(k, \tau) = \pr(S_{\tau} = k) \sim
    Binomial(\frac{\tau}{\delta}, \frac{1}{N})$ and by the Poissoin
    approximation of the Binomial for small $p$ and large $n$ $\pr(k,
    \tau)~Poisson(np)=Poisson(\frac{\tau}{N\delta})$. Notice that
    since $N\delta = \frac{1}{\lambda }$ we have $\tau \cdot \frac{1}{N\delta}
    = \lambda \tau$ thus the number of arrivals in an interval of length $\tau$
    is $Poisson(\lambda \tau)$. \\\\ Even cooler yet, the memoryless property
    of Poisson reads off directry from the iid-ness from Bernoulli RVs. Given
    we have already seen $t$ arrivals, the probability of seeing $s+t$ arrivals
    is the same as seeing $s$ arrivals. This follows directly from the poisson.
    But also not that  since the Bernoulli's are iid each of the additional $s$
    success are not in any way affected by the history of $t$ arrivals. The
    waiting time until the next arrival is just the waiting time for
    a Bernoulli to output 1, but that is just geometric and the continous
    analog is exponential. Given that we have $k$ arrivals in an interval of
    length $\tau$ its easy to see that the distribution of arrivals is uniform.
    Think of the discretized version. If we know that in $\tau$ iid bernoulli RVs
    there were $k$ successes and nothing else, then the distribution on the indices of the bernoulli's which were
    successful is uniform on $[1, \tau]$. 
\newpage
\section{Markov Chains}
Interested in models where the effect of the past on the future is summarized
by a state, which changes over time given probabilities.
\subsection{Discrete Time Markov Chains}
In \textbf{discrete-time Markov chains}, state changes at certain discrete time
instants, indexed by an integer variable $n$. At each step $n$, the state of
the chain is denoted $X_{n}$, and belongs to a \textbf{finite} set $\mathcal{S}$ of
possible states, called the state space. WLOG let $S = \{1,\ldots,m\}$.The Markov Chain is described in terms
of its transition probabilities $p_{ij}$ where \[
  p_{ij} = \pr(X_{n+1} = j | X_{n}=i) ~~ i,j \in \mathcal{S}
\] 
The key assumption underlying these chains is that the transition probabilities
apply whenever $i$ is visited, no matter what happened in the past, and no
matter how $i$ was reached, formally this is the \textbf{Markov property},
requiring that: \[
  \pr(X_{n+1} = j | X_{n}=i, X_{n-1}=i_{n-1},\ldots,X_0=i_0) = \pr(X_{n+1}=j
  | X_{n}=i) = p_{ij}
\] Furthermore the transition probabilities $p_{ij}$ must be nonnegative, and
sum to one: \[
  \sum_{j=1}^{m}  p_{ij} = 1, ~\text{for all }i
\] 
Another efficient way to encode the MC chain model is a transition probability
matrix, a 2D array whose element at row $i$ and column $j$ is $p_{ij}$, the
transition probability from $i$ to $j$ \[
P = \begin{bmatrix}
  p_{11} & p_{12} & \cdots & p_{1m} \\
  p_{21} & p_{22} & \cdots & p_{2m} \\
  \vdots & \vdots & \vdots & \vdots \\
  p_{m 1} & p_{m 2} & \cdots & p_{mm}
\end{bmatrix}
\] Note that the transition matrix format is (row, col), $(i,j)$, (from,
to).\\\\There is often a need to introduce new states that capture the depedence of the
future on the model's past history. The probability of any single path can be
found simply using the multiplication rule and tracing the path of transition
probabilities. If there is no conditioning on the first state then we need to
specify a probability law for the initial state $X_0$, the initial
distribution.
\subsubsection{$n$-Step Transition Probabilities}
Many problems require calculating the probability law of the state at some
future tiem, conditioned on the current state. This probability law is captured
by the $n$\textbf{-step transition probabilities}, defined by  \[
  r_{ij}(n) = \pr(X_{n}=j | X_0 = i)
\] In words, $r_{ij}(n)$  is the probability that the state after $n$  time
periods will be $j$, give that the current state is $i$. We can calculate it
using the following recursion, \textbf{Chapman-Kolmogorov equation}  
\begin{theorem}[Chapman-Kolmogorov Equation for $n$-Step Transition
  Probabilities]
  The $n$ -step transition probabilities can be calculated using the formula \[
    r_{ij}(n) = \sum_{k=1}^{m} r_{ik}(n-1)p_{kj}, ~~~n>1, \text{and all }i,j
  \] starting with $r_{ij}(1) = p_{ij}$
  \begin{proof}
    \begin{align*}
      \pr(X_{n}= j | X_0 = i) &= \sum_{k=1}^{m} \pr(X_{n-1}=k | X_0
      =i)\pr(X_{n} = j | X_{n-1}=k, X_0=i) \\ &= \sum_{k=1}^{m} r_{ik}(n-1)
    \end{align*}
  \end{proof}
The Chapman-Kolmogorov equation can be represented my concisely via matrix
multiplication, specifically the matrix of $n$-step transition probabilities
$r_{ij}(n)$ is obtained by multiplying the matrix of $(n-1)$-step transition
probabilities $r_{ik}(n-1)$, with the one-step transition probability matrix.
Thus the $n$-step transition probability matrix is just the $n$th power of the
transition probability matrix, $P^{n} $.
\end{theorem}
As $n\to \infty$, if $r_{ij}(n)$ converges to a limit and this limit does not
depend on initial state $i$, then we say that state $j$ has a positive
""steady-state" probability of being occupied at times far into the future.
However, there are examples of qualitatively different behavior: where
$r_{ij}(n)$ converges, but the limit depends on the initial state, and can be
zero for selected states, particularly the probability that a particular
absorbing state will be reached depends on how "close" we start to that state.
This illustrates that there is a variety of states and asymptotic occupancy
behavior in Markov chains. Thus we are motivated to classify and analyze
various possibilities. 
\subsection{Classification of States}
We begin by focusing on the mechanism by which some states after being visited
once, are certain to be visited again, while for other states this may not be
the case. Our goal is to classify the states of a Markov chain with a focus on
the long-term frequency by which they are visited. Let us first make the notion
of revisiting a state precise. A state $j$ is \textbf{accessible} from state $i$
if for some $n$, the $n$-step transition probability $r_{ij}(n)$ is positive
(there is a positive probability of reaching $j$, starting from $i$, after some
number of time steps). Let $A(i)$ denote the set of states accessible from
state $i$.
\begin{definition}[recurrent]
  State $i$ is \textbf{recurrent} if \emph{for every} $j$ that is accessible
from $i$, $i$ is also accessible from $j$, that is $\forall j \in A(i)$, $i\in
A(j)$. 
\end{definition}
If we start at a recurrent state $i$, we can only visit $j\in A(i)$. But since
state $i$ is recurrent $i\in A(j)$. Thus, from any future state, there always
some probability of returning to state $i$. Given enough time, this is certain
to happen. By repeating this argument, if a recurrent state is visited once, it
is certain to be revisited an infinite number of times. \textcolor{red}{does
this hold only if state $j$ is itself recurrent? like what if you transition to
state $j$ but the from state $j$ you can transition into some absorbing state
$k$, then there is a nonzero probabilty of not returning to state $i$, which
means that you are note certain to revist $i$ an infinite number of times}.\\\\
A state is called \textbf{transient} if it is not recurrent. Thus, a state
$i$ is transient if there is a state $j\in A(i)$ such that $i$ is not
accessible from $j$, that is there exists $j\in A(i)$ but $i\\not\in  A(j)$.
After each visit to state $i$, there is a positive probability that the state
enteres such a state $j$ from which $i$ is no longer accessible. Given enough
time, this will hapen, and state $i$ cannot be visited after that. Thus
a transiet state will only be viisted a a finite number of times. 
\begin{definition}[recurrent class]
  If $i$ is a recurrent state, the set of states $A(i)$ that are accessible
  form a \textbf{recurrent class} (or simply \textbf{class}). This means that
  the states in $A(i)$ are all accessible from each other, and no state outside
  $A(i)$ is accessible from them. Mathematically, for a recurrent state $i$, we
  have $A(i) = A(j)$ for all $j$ that belong to $A(i)$. 
\end{definition}
At least one recurrent state must be accessible from any transient state, this
follows from the definition of transient state. It follows that there must
exist at least one recurrent state and hence at least one class, giving the
following result
\begin{theorem}[Markov Chain Decomposition]
  \begin{itemize}
    \item A MC can be decomposed into one or more recurrent classes, plus
      possibly some transient states
      \item  A recurrent state is accessible for all states in its class, but
        is not accesssible from \emph{recurrent} states in \emph{other classses}
        \item  A transient state is not accessible from any recurrent state,
          but a a recurrent state must be accessible from a transient state
          (otherwise the state cannot be transient it would just be recurrent
          with itself)
          \item  At least one, possibly more, recurrent states are accessible
            from a given transiet state, this follows directly from the
            previous bullet 
  \end{itemize}
\end{theorem}
The previous theorem implies the following:
\begin{enumerate}
  \item  once the state enters (or starts in) a class of recurrent states, it
    stays within that class; since all states in the recurrence class are
    accessible from each other, all states in the class will be visited
    an infinite number of times
    \item  if the initial state is transient, then the state trajectory
      contains an initial portion consisting of transient states and a final
      portion consisting of recurrent states from the same class
\end{enumerate}
\begin{example}
  In a MC at least one recurrent state must be accssible from any given state.
  That is, for any $i$, there is at least one recurrent $j$ in the set $A(i)$. 
  \begin{proof}
    \textcolor{red}{TODO} 
  \end{proof}
\end{example}
\begin{example}
  Show that if a recurrent state is visited once, the probability that it will
  be visited gain in the future is equal to 1 (and, therefore, the probability
  that it will be visited an infinte number of times is equal to 1).
  \begin{proof}
    Let $s$ be a recurrent state, and suppoes $s$ has been visited once. From
    then on, the only possible states are those in the same recurrence class as
    $s$. Therefore WLOG we can assume there is a single self-absorbing
    state $s$. We now want to show from some current state $i\neq s$,
    $s$ is guaranteed to be visited some time in the future.\\\\ Consider a new
    MC with a self-absorbing state $s$. The transitions out of states $i$,
    $i\neq s$ are unaffected. Clearly, $s$ is recurrent in the new chain.
    Furthermore, for any $i\neq s$, there is a positive probability path from
    $i$ to $s$ in the original chain (since $s$ is recurrent in the original
    chain), this holds true in the new chain \textcolor{red}{still not
    completely convinced of this}. Since $i$ is not accessible from $s$ in the
    new chain, it follows that  every $i\neq s$ in the new chain is transient.
    But if there exist recurrent states in a MC, they will eventually be
    visited, so the state $s$ will eventually be visited by the new chain (with
    probability 1). But the original chain is identical to the new one until
    the time that $s$ is first visited. Hence, state $s$ is guaranteed to be
    eventually visited by the original chain. By repeating this argument, we
    see that $s$ is guaranteed to be visited an infinite number of times (with
    probability 1).
  \end{proof}
\end{example}

\subsubsection{Periodicity}
We are now interested in a characterization of a recurrent class, which relates
to the presence or absence of a certain periodic pattern in the times that
a state can be visited. 
\begin{definition}[periodic]
  A recurrent class is said to be periodic if the states can be grouped in
  $d>1$ disjoint subsets $S_1,\ldots,S_{d}$ so that  all transitions from one
  subset lead to the next subset. A recurrent class that is not period is said
  to be aperiodic.
\end{definition}
In a periodic recurrent class we move through the sequence of subsets in order,
and after $d$ steps, we end up in the same subset. Given a periodic recurrent
class and a time $n$, and a state $i$, there must exist one or more states $j$
for which $r_{ij}(n)=0$. This is because starting from $i$, only one of the
sets $S_{k}$ is a possible transition at time $n$. Thus to verify aperiodicity
of a recurrent class $R$, is to check whether there is a time $n\geq 1$ and
a state $i\in R$ from which all states $j \in R$ can be reached in $n$ steps, that
is $r_{ij}(n)>0, ~\forall j\in R$. A converse is true as well: if a recurrent
class $R$ is aperiodic, then there exists a time $n$such that $r_{ij}(n) >0,
~\forall i,j \in R$. Another way to check periodicity within a recurrent class
is to calculate the gcd of all length paths from a state back to itself. If the
gcd is 1 then the class is aperiodic. Equivalently, if there is any state with
a self-loop in the recurrent class, then the recurrence class must be
aperiodic.
\subsection{Steady-State Behavior}
We wish to understand long-term state occupancy behavior, the $n$-step
transition probabilities $r_{ij}(n)$ when $n$ is very large. We are interested
in understanding when $r_{ij}(n)$ convereges to steady-state values that are
independent of the initial state. If there are multiple recurrence classes,
then the limiting values of $r_{ij}(n)$ depend on the initial state. Thus we
restrict our attention to chains involving a single recurrent class, plus some
transient states and we can generalize the asymptotic behavior of multiclass
chains in terms of the asymptotic behavior of single-class chains.\\\\ A single
recurrent class is not strong enough to guarantee convergence of $r_{ij}(n)$,
consider a recurrence class with two states 1 and 2 where $p_{12} = p_{21}=1$
observe $r_{ij}(n)$ generically oscillates. \\\\ Thus if we exclude
multiple recurrent classes and/or a periodic class, for every state $j$ the
probability $r_{ij}(n)$, of being at state $j$, approaches a limiting value
$\pi_{j}$ that is independent of the initial state $i$ with the following
interpretation \[
  \pi_{j} \approx \pr(X_{n}=j), ~~ \text{when $n$ is large}
\] 
and is called the \textbf{steady-state probability of } $j$ . This is
consolidated in the following important theorem 
\begin{theorem}[Steady-State Convergence Theorem]
  COnsider a MC with a single recurrent class, which is periodic. Then, the
  states $j$ are associated with steady-state probabilities $\pi_{j}$  that have
  the following properties:
  \begin{enumerate}[label=(\alph*)]
      \item For each $j$ , we have \[
          \lim_{n\to \infty}r_{ij}(n)= \pi_{j}, ~~ \forall i
      \]
      \item The $\pi_{j}$  are the unique solution to the system of equations
        below:
        \begin{align*}
          \pi_{j} &= \sum_{k=1}^{m} \pi_{k}p_{kj}, ~~ j=1,\ldots, m, \\
          1 &= \sum_{k=1}^{m} \pi_{k}
        \end{align*}
        \item We have \begin{align*}
            \pi_{j} = 0, ~ \text{for all transient states $j$}, \\
            \pi_{j} > 0, ~ \text{for all recurrent states $j$}
        \end{align*}
  \end{enumerate}
\end{theorem} 
The steady-state probabilities $\pi_{j}$ sum to 1 and form
a probabilty-distribution $\pi$ on the state space, called the \textbf{stationary
distribution}  of the chain with the property that if the intial state is
chosen according to the distrubution $\pi$ then \[
\pi P^{n}  = \pi, ~~ \forall n \in \Z_{\geq 0}
\] Thus if the initial state is chosen according to the stationary
distribution, the state at any future time will have the same distribution. The
equations \[
 \pi_{j} &= \sum_{k=1}^{m} \pi_{k}p_{kj}, ~~ j=1,\ldots, m,
\] are called the \textbf{balance equations}.
\begin{theorem}[Steady-State Probabilities as Expected State Frequencies]
 For a MC with a single class which is aperiodic, the steady-state
 probabilities $\pi_{j}$ satisfy \[
   \pi_{j} = \lim_{n \to \infty} \frac{v_{ij}(n)}{n}
 \]  where $v_{ij}(n)$ is the expected value of the number of visits to state
 $j$ within the first $n$ transitions, starting from state $i$. 
\end{theorem}
\begin{theorem}[Expected Frequency of a Particular Transition]
  Consider $n$ transitions of a MC with a single class which is aperiodic,
  starting from an initial state. Let $q_{jk}(n)$ be the expected number of
  such transitions that take the state from $j$ to $k$. Then, regardless of the
  initial state we have \[
    \lim_{n\to \infty} \frac{q_{jk}(n)}{n} = \pi_{j}p_{jk}
  \] That is, the expected frequency of transitions from $j$ to $k$, regardless
  of initial state converges to $\pi_{j}p_{jk}$.
\end{theorem}
\subsection{Birth-Death Process}
\begin{definition}[birth-death process]
 A birth-death process is a MC in which the states are linearly arranged and
 transitions can only occur to a neighboring state, or else leave the state
 unchanged (self-loop).  
\end{definition}
Birth-death processes also induce the notation \[
  b_{i} = \pr(X_{n+1} = i+1 | X_{n} = i), ~~~\text{"birth" probabilty at state
  $i$}
\]\[
d_{i} = \pr(X_{n+1} = i-1 | X_{n} = i), ~~~\text{"death" probabilty at state $i$}
\] 
For a b-d process the balance equations are substanitally simplified. Any
transition from $i$ to $i+1$ has to be followed by a transition from $i+1$ to
$i$ before another transition from $i$ to $i+1$ to occur. Thus the expected
frequency of transitions from $i$ to $i+1$, which is $\pi_{i}b_{i}$  must be
equal to the expected frequency of transitions from $i+1$ to $i$ which is
$\pi_{i+1}d_{i+1}$, leading us to the \textbf{local balance} equations \[
\pi_{i}b_{i} = \pi_{i+1}d_{i+1}, ~~~ i=0,1,\ldots,m-1
\]  Using these local balance equations we obtain \[
\pi_{i} = \pi_0 \frac{b_0b_1\cdots b_{i-1}}{d_1d_2\cdots d_{i}}, ~~~
i=1,\ldots,m
\] using this and the normalization equation we can compute the probabilities
$\pi_{i}$. 

\subsection{Absorption Probabilities and Expected Time to Absorption}

\subsection{Recurrence and Transience}
For each $x\in \mathcal{X} $, the random variable $T_{x} $ represents the first
time that the chain visits state $x$, that is $T_{x}:= \min \{n\in \N: X_{n}
  = x $. $T_{x} $ is called the hitting time of state $x$. We further define
  $T_{x}^{+}  $ which is the hitting time for state $x$ execept $T_{x}^{+}
  $ cannot equal 0, to avoid the trivial condition in which the chain starts at
  $x$. 
  Let $\rho_{x,y}:=\pr_x(T_{y}^{+} < \infty  )$, the probability that starting
  from state $x$ we eventually reach state $y$ and shorthand
  $\rho_{x}=\rho_{x,x}  $. 
  \begin{definition}
    A state $x$ is \textbf{recurrent}  if $\rho_{x}=1 $ and \textbf{transient}
  if $\rho_{x}<1 $.
  \end{definition}
  \begin{proposition}
    Let $N_{x} $ be the total number of visits to state $x$. If $x$ is
    recurrent then $N_{x}=\infty $ in probability almost surely. Thus
    $\mathbb{E}_{x} [N_{x} ] = \mathbb{E} [N_{x} | X_{0}=x  ] =\infty$. On
    the other hand, if $x$ is transient then $\mathbb{E}_{x} [N_{x}
    ] < \infty$, in fact, \[
      \mathbb{E}_{x} [N_{x} ]  = \frac{\rho_{x}}{1-\rho_{x}}
    \]  
  \end{proposition}
\end{document}
